{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dddf8162-ffc5-4e02-a7b3-68d0e1246bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# # Spark Python Home ***\n",
    "\n",
    "# \t$ vi $SPARK_HOME/conf/spark-env.sh\n",
    "\t\n",
    "# # CONDA & PYTHON\n",
    "# export CONDA_HOME=/opt/miniconda3/bin\n",
    "# export PYTHON_HOME=${CONDA_HOME}/python\n",
    "# export PYSPARK_PYTHON=${PYTHON_HOME}\n",
    "# export PYSPARK_DRIVER_PYTHON=${PYTHON_HOME}\n",
    "\n",
    "# export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "# export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)\n",
    "# export LD_LIBRARY_PATH=$JAVA_LIBRARY_PATH\n",
    "# export SPARK_CLASSPATH=$SPARK_CLASSPATH:/usr/local/spark/jars/*\n",
    "\n",
    "# export SPARK_MASTER_IP=localhost\n",
    "# export SPARK_MASTER_PORT=7077\n",
    "# export SPARK_WORKER_MEMORY=4g\n",
    "#\n",
    "################################################\n",
    "# # .bashrc\n",
    "\n",
    "#  $ vi ~/.bashrc\n",
    "#\n",
    "# JAVA ENV SETTINGS\n",
    "\n",
    "# # export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n",
    "# export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "\n",
    "\n",
    "# export JRE_HOME=${JAVA_HOME}/jre\n",
    "# export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\n",
    "# export MYSQL_LIB=${JRE_HOME}/lib/ext/mysql\n",
    "# export PATH=${JAVA_HOME}/bin:$PATH\n",
    "\n",
    "# # HADOOP ENV SETTINGS\n",
    "# export HADOOP_HOME=/usr/local/hadoop\n",
    "# export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\n",
    "# export JAVA_LIBRARY_PATH==${HADOOP_HOME}/lib/native\n",
    "# export PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH\n",
    "\n",
    "# # SPARK EVN SETTINGS\n",
    "# export SPARK_HOME=/usr/local/spark\n",
    "# export PATH=${SPARK_HOME}/bin:$PATH\n",
    "# # Conflict with Hadoop: start-all.sh\n",
    "# # export PATH=${SPARK_HOME}/sbin:$PATH\n",
    "\n",
    "# # CONDA & PYTHON\n",
    "# export CONDA_HOME=/opt/miniconda3/bin\n",
    "# export PYTHON_HOME=${CONDA_HOME}/python\n",
    "# export PYSPARK_PYTHON=${PYTHON_HOME}\n",
    "# export PYSPARK_DRIVER_PYTHON=${PYTHON_HOME}\n",
    "\n",
    "# export PYSPARK_SUBMIT_ARGS=\"--master local[2] pyspark-shell\"\n",
    "#\n",
    "# # source ~/.bashrc\n",
    "################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f41cf64-565f-48b4-a5e5-4d48fc404270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the PySpark environment variables\n",
    "# FOR \n",
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/usr/local/spark\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master local[2] pyspark-shell\"\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages mysql:mysql-connector-j-9.0.0.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcf3ef9b-c33b-4512-bc23-f4fd53214e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()  # 初始化Spark\n",
    "\n",
    "# $ ./bin/spark-shell --driver-class-path postgresql-9.4.1207.jar \n",
    "#                     --jars postgresql-9.4.1207.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e53b0254-a8c6-4fdb-b9ce-08ef5608d1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+---+\n",
      "| id|                name|gender|age|\n",
      "+---+--------------------+------+---+\n",
      "|  1|Xueqian             |  F   | 23|\n",
      "|  2|Weiliang            |  M   | 24|\n",
      "|  3|Niro                |  F   | 23|\n",
      "+---+--------------------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySQL Connection\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark/jars/mysql-connector-j-9.0.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# url = \"jdbc:mysql://localhost:3306/spark\"\n",
    "url = \"jdbc:mysql://localhost/spark\"\n",
    "# URL=jdbc:mysql://[host][:port]/[database]\n",
    "# mysql> SHOW VARIABLES WHERE Variable_name = 'hostname';\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"Smart_Battery@2022\"\n",
    "}\n",
    "\n",
    "df = spark.read.jdbc(url=url, table=\"student\", properties=properties)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "823fb79a-ad2c-454e-b2fd-51d0f61a2a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "studentRDD = spark.sparkContext.parallelize([\"3 Rongcheng M 26\",\"4 Guanhua M 27\"]).map(lambda line : line.split(\" \"))\n",
    "\n",
    "## 设置模式信息\n",
    "schema = StructType([StructField(\"name\", StringType(), True),StructField(\"gender\", StringType(), True),StructField(\"age\",IntegerType(), True)])\n",
    "rowRDD = studentRDD.map(lambda p : Row(p[1].strip(), p[2].strip(),int(p[3])))\n",
    "\n",
    "\n",
    "## 建立起Row对象和模式之间的对应关系，也就是把数据和模式对应起来\n",
    "\n",
    "studentDF = spark.createDataFrame(rowRDD, schema)\n",
    "prop = {}\n",
    "prop['user'] = 'root'\n",
    "prop['password'] = 'Smart_Battery@2022'\n",
    "prop['driver'] = \"com.mysql.jdbc.Driver\"\n",
    "studentDF.write.jdbc(\"jdbc:mysql://localhost:3306/spark\",'student','append', prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a146415f-5c88-4709-aaec-09f3c51d6d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+---+\n",
      "| id|                name|gender|age|\n",
      "+---+--------------------+------+---+\n",
      "|  1|Xueqian             |  F   | 23|\n",
      "|  2|Weiliang            |  M   | 24|\n",
      "|  3|Niro                |  F   | 23|\n",
      "|  4|Guanhua             |  M   | 27|\n",
      "|  5|Rongcheng           |  M   | 26|\n",
      "+---+--------------------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84111931-d206-4cc5-853a-c426ddbc8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71338acb-b512-47e7-9e1f-49d1987b16e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
